{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wankh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#!pip install mysql-connector-python\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV files and their corresponding table names\n",
    "csv_files = [\n",
    "   (\"customers.csv\", \"customers\"),\n",
    "   (\"orders.csv\", \"orders\"),\n",
    "   (\"sellers.csv\", \"sales\")\n",
    "   (\"products.csv\", \"products\"),\n",
    "   (\"geolocation.csv\", \"geolocation\"),\n",
    "   (\"payments.csv\", \"payments\") , \n",
    "   (\"order_items.csv\",\"order_items\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('orders.csv', 'orders')]\n"
     ]
    }
   ],
   "source": [
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added sql details for mysql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='1234',\n",
    "    database='sales_analysis'\n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing orders.csv\n",
      "NaN values before replacement:\n",
      "order_id                            0\n",
      "customer_id                         0\n",
      "order_status                        0\n",
      "order_purchase_timestamp            0\n",
      "order_approved_at                 160\n",
      "order_delivered_carrier_date     1783\n",
      "order_delivered_customer_date    2965\n",
      "order_estimated_delivery_date       0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Folder containing the CSV files\n",
    "folder_path = 'archive (1)'\n",
    "\n",
    "def get_sql_type(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'INT'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'FLOAT'\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return 'BOOLEAN'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return 'DATETIME'\n",
    "    else:\n",
    "        return 'TEXT'\n",
    "\n",
    "for csv_file, table_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Replace NaN with None to handle SQL NULL\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    # Debugging: Check for NaN values\n",
    "    print(f\"Processing {csv_file}\")\n",
    "    print(f\"NaN values before replacement:\\n{df.isnull().sum()}\\n\")\n",
    "\n",
    "    # Clean column names\n",
    "    df.columns = [col.replace(' ', '_').replace('-', '_').replace('.', '_') for col in df.columns]\n",
    "\n",
    "    # Generate the CREATE TABLE statement with appropriate data types\n",
    "    columns = ', '.join([f'`{col}` {get_sql_type(df[col].dtype)}' for col in df.columns])\n",
    "    create_table_query = f'CREATE TABLE IF NOT EXISTS `{table_name}` ({columns})'\n",
    "    cursor.execute(create_table_query)\n",
    "    \n",
    "    # Insert DataFrame data into the MySQL table\n",
    "for _, row in df.iterrows():\n",
    "        # Convert row to tuple and handle NaN/None explicitly\n",
    "        values = tuple(None if pd.isna(x) else x for x in row)\n",
    "        sql = f\"INSERT INTO `{table_name}` ({', '.join(['`' + col + '`' for col in df.columns])}) VALUES ({', '.join(['%s'] * len(row))})\"\n",
    "        cursor.execute(sql, values)\n",
    "\n",
    "    # Commit the transaction for the current CSV file\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : if any file is not being dumped in my sql you can directly dump the file buy\n",
    "selcet the database -------> tabel -------->table import wizrad option -------> location for the csv file which you want to convert .\n",
    "The reason why py is use here is to smooth the proocess the file dummping procces as it repetitive and could take time ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
